{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57646cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aneeshsathe/miniconda3/envs/torch/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/aneeshsathe/miniconda3/envs/torch/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <47DCCE5B-5493-3979-96BF-5E3CEE06843B> /Users/aneeshsathe/miniconda3/envs/torch/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <B5DCF36B-AC07-3EF2-A997-D38A25D2CCE2> /Users/aneeshsathe/miniconda3/envs/torch/lib/python3.8/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed254043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(doubleConvolution, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b400a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 3, features = [64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.down_conv = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(stride = 2, kernel_size = 2)\n",
    "        \n",
    "        for feature in features:\n",
    "            self.down_conv.append(doubleConvolution(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        for feature in reversed(features):\n",
    "            self.up_conv.append(nn.ConvTranspose2d(feature*2, feature, kernel_size = 2, stride = 2))\n",
    "            self.up_conv.append(doubleConvolution(feature*2, feature))\n",
    "        \n",
    "        self.bottomConv = doubleConvolution(features[-1], features[-1]*2)\n",
    "        \n",
    "        self.lastConv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.down_conv:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottomConv(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for i in range(0, len(self.up_conv), 2):\n",
    "            x = self.up_conv[i](x)\n",
    "            skip_connection = skip_connections[i//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size = skip_connection.shape[2:])\n",
    "            concat = torch.cat((skip_connection, x), dim = 1)\n",
    "            x = self.up_conv[i+1](concat)\n",
    "\n",
    "        x = self.lastConv(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a44ff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aneeshsathe/miniconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 480, 360])\n",
      "torch.Size([16, 3, 480, 360])\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "x = torch.rand([16, 3, 480, 360])\n",
    "pred = model(x)\n",
    "print(pred.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb2ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
